{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e86cb83",
   "metadata": {},
   "source": [
    "## Reference Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2668f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=o-pZk5R0TZg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46e0cc2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b5348d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9258b6",
   "metadata": {},
   "source": [
    "## Import the data, as well as split into a smaller/larger dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42be3f69",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m      7\u001b[0m df_examples_products \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(\n\u001b[1;32m      8\u001b[0m     df_examples,\n\u001b[1;32m      9\u001b[0m     df_products,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     right_on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_locale\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Smaller dataset: filter to U.S. products\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m df_task_1 \u001b[38;5;241m=\u001b[39m df[\n\u001b[1;32m     17\u001b[0m     (df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmall_version\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m\n\u001b[1;32m     18\u001b[0m     (df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_locale\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mus\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m&\u001b[39m\n\u001b[1;32m     19\u001b[0m     (df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_title\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnotnull())\n\u001b[1;32m     20\u001b[0m ]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Task 2: larger dataset\u001b[39;00m\n\u001b[1;32m     23\u001b[0m df_task_2 \u001b[38;5;241m=\u001b[39m df_examples_products[df_examples_products[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlarge_version\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Source code: https://github.com/amazon-science/esci-data/blob/main/README.md\n",
    "\n",
    "df_examples = pd.read_parquet('shopping_queries_dataset_examples.parquet')\n",
    "df_products = pd.read_parquet('shopping_queries_dataset_products.parquet')\n",
    "df_sources = pd.read_csv(\"shopping_queries_dataset_sources.csv\")\n",
    "\n",
    "df_examples_products = pd.merge(\n",
    "    df_examples,\n",
    "    df_products,\n",
    "    how='left',\n",
    "    left_on=['product_locale','product_id'],\n",
    "    right_on=['product_locale', 'product_id']\n",
    ")\n",
    "\n",
    "# Smaller dataset: filter to U.S. products\n",
    "df_task_1 = df[\n",
    "    (df[\"small_version\"] == 1) &\n",
    "    (df[\"product_locale\"] == \"us\") &\n",
    "    (df[\"product_title\"].notnull())\n",
    "]\n",
    "\n",
    "# Task 2: larger dataset\n",
    "df_task_2 = df_examples_products[df_examples_products[\"large_version\"] == 1]\n",
    "df_task_2_train = df_task_2[df_task_2[\"split\"] == \"train\"]\n",
    "df_task_2_test = df_task_2[df_task_2[\"split\"] == \"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c5f4db",
   "metadata": {},
   "source": [
    "## Initial Model: Smaller sample set, only looking at exact matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb20f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task_1 = df_task_1[\n",
    "    (df_task_1[\"split\"] == \"train\")\n",
    "]\n",
    "df_task_1 = df_task_1[df_task_1['esci_label'] == 'E']\n",
    "\n",
    "sample_df = df_task_1.copy()\n",
    "queries = sample_df['query'].values\n",
    "products = sample_df['product_title'].values\n",
    "\n",
    "# Vectorize Queries and Products\n",
    "vectorizer = TfidfVectorizer()\n",
    "query_vecs = vectorizer.fit_transform(queries)\n",
    "product_vecs = vectorizer.transform(products)  # Use same vocab\n",
    "\n",
    "# Score & Recommend \n",
    "for i, query in enumerate(queries):\n",
    "    scores = cosine_similarity(query_vecs[i], product_vecs).flatten()\n",
    "    ranked_indices = scores.argsort()[::-1]\n",
    "    print(f\"\\nTop matches for: '{query}'\")\n",
    "    for idx in ranked_indices[:3]:\n",
    "        print(f\"- {products[idx]} (score: {scores[idx]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e253a18",
   "metadata": {},
   "source": [
    "## Incorporate Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e29bba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Smaller Sample + Combine Product Metadata\n",
    "sample_df = df_task_1.sample(n=20, random_state=42).copy()\n",
    "sample_df['full_product_text'] = (\n",
    "    sample_df['product_title'].fillna('') + ' ' +\n",
    "    sample_df['product_description'].fillna('') + ' ' +\n",
    "    sample_df['product_brand'].fillna('')  + ' ' + train_df['product_bullet_point'].fillna('')\n",
    ")\n",
    "\n",
    "queries = sample_df['query'].values\n",
    "products = sample_df['full_product_text'].values\n",
    "\n",
    "# Use SentenceTransformer for Semantic Embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "query_vecs = model.encode(queries, convert_to_tensor=True)\n",
    "product_vecs = model.encode(products, convert_to_tensor=True)\n",
    "\n",
    "# Score, Normalize, Recommend\n",
    "for i in range(len(queries)):\n",
    "    scores = cosine_similarity(query_vecs[i].cpu().numpy().reshape(1, -1),\n",
    "                               product_vecs.cpu().numpy()).flatten()\n",
    "    # Normalize scores between 0 and 1\n",
    "    scores = (scores - scores.min()) / (scores.max() - scores.min() + 1e-8)\n",
    "\n",
    "    ranked_indices = scores.argsort()[::-1]\n",
    "    print(f\"\\nTop matches for: '{queries[i]}'\")\n",
    "    for idx in ranked_indices[:3]:\n",
    "        print(f\"- {sample_df.iloc[idx]['product_title']} (score: {scores[idx]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6720325d",
   "metadata": {},
   "source": [
    "## Incorporate training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205c334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_task_1_train.sample(n=30, random_state=42).copy()\n",
    "train_df['full_product_text'] = (\n",
    "    train_df['product_title'].fillna('') + ' ' +\n",
    "    train_df['product_description'].fillna('') + ' ' +\n",
    "    train_df['product_brand'].fillna('') + ' ' + train_df['product_bullet_point'].fillna('')\n",
    ")\n",
    "train_products = train_df['full_product_text'].values\n",
    "train_titles = train_df['product_title'].values  # for display\n",
    "\n",
    "# Prepare Test Queries \n",
    "test_df = df_task_1_test.sample(n=5, random_state=24).copy()\n",
    "test_queries = test_df['query'].values\n",
    "\n",
    "# Embeddings \n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "product_vecs = model.encode(train_products, convert_to_tensor=True)\n",
    "query_vecs = model.encode(test_queries, convert_to_tensor=True)\n",
    "\n",
    "# Recommend Products for Each Query\n",
    "for i, query in enumerate(test_queries):\n",
    "    scores = cosine_similarity(\n",
    "        query_vecs[i].cpu().numpy().reshape(1, -1),\n",
    "        product_vecs.cpu().numpy()\n",
    "    ).flatten()\n",
    "\n",
    "    # Normalize scores between 0 and 1\n",
    "    scores = (scores - scores.min()) / (scores.max() - scores.min() + 1e-8)\n",
    "    ranked_indices = scores.argsort()[::-1]\n",
    "\n",
    "    print(f\"\\nTop matches for test query: '{query}'\")\n",
    "    for idx in ranked_indices[:3]:\n",
    "        print(f\"- {train_titles[idx]} (score: {scores[idx]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d52aae",
   "metadata": {},
   "source": [
    "## Trying to use Recall and MRR (This doesn't work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c59072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Filter for US, Exact Matches, Titles Present ===\n",
    "df_task_1 = df[\n",
    "    (df[\"small_version\"] == 1) &\n",
    "    (df[\"product_locale\"] == \"us\") &\n",
    "    (df[\"product_title\"].notnull()) &\n",
    "    (df[\"esci_label\"] == \"E\")\n",
    "]\n",
    "\n",
    "# === Full Train and Test Sets ===\n",
    "df_train = df_task_1[df_task_1[\"split\"] == \"train\"].copy()\n",
    "df_test = df_task_1[df_task_1[\"split\"] == \"test\"].sample(n=20, random_state=24).copy()\n",
    "\n",
    "# === Build full product metadata ===\n",
    "df_train['full_product_text'] = (\n",
    "    df_train['product_title'].fillna('') + ' ' +\n",
    "    df_train['product_description'].fillna('') + ' ' +\n",
    "    df_train['product_brand'].fillna('')\n",
    ")\n",
    "\n",
    "# === SentenceTransformer Embeddings ===\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "product_vecs = model.encode(df_train['full_product_text'].values, convert_to_tensor=True)\n",
    "product_ids = df_train['product_id'].values\n",
    "\n",
    "query_vecs = model.encode(df_test['query'].values, convert_to_tensor=True)\n",
    "true_ids = df_test['product_id'].values\n",
    "\n",
    "# === Evaluate Recall@3 and MRR@3 ===\n",
    "k = 3\n",
    "recall_hits = 0\n",
    "reciprocal_ranks = []\n",
    "\n",
    "for i in range(len(df_test)):\n",
    "    scores = cosine_similarity(\n",
    "        query_vecs[i].cpu().numpy().reshape(1, -1),\n",
    "        product_vecs.cpu().numpy()\n",
    "    ).flatten()\n",
    "\n",
    "    ranked_indices = scores.argsort()[::-1][:k]\n",
    "    retrieved_ids = product_ids[ranked_indices]\n",
    "\n",
    "    print(f\"\\nüîç Query: {df_test.iloc[i]['query']}\")\n",
    "    print(\"Top retrieved product IDs:\", list(retrieved_ids))\n",
    "    print(\"True product ID:\", true_ids[i])\n",
    "\n",
    "    if true_ids[i] in retrieved_ids:\n",
    "        recall_hits += 1\n",
    "        rank = list(retrieved_ids).index(true_ids[i]) + 1\n",
    "        reciprocal_ranks.append(1.0 / rank)\n",
    "    else:\n",
    "        reciprocal_ranks.append(0.0)\n",
    "\n",
    "recall_at_k = recall_hits / len(df_test)\n",
    "mrr_at_k = np.mean(reciprocal_ranks)\n",
    "\n",
    "print(f\"\\n‚ú® Evaluation Results (Full Train Set):\")\n",
    "print(f\"Recall@{k}: {recall_at_k:.4f}\")\n",
    "print(f\"MRR@{k}:    {mrr_at_k:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d050ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
